{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c610a2d-4d05-4b3d-8c69-23e06d699df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import glob\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd41de8-cb1d-4b85-a284-5393a9ac2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: data setup + super-charged augmentations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# Custom resize + padding to fixed portrait shape\n",
    "class ResizePad:\n",
    "    def __init__(self, size, fill_mean=(0.485, 0.456, 0.406)):\n",
    "        self.W, self.H = size\n",
    "        r, g, b = (int(c*255) for c in fill_mean)\n",
    "        self.fill = (r, g, b)\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        scale = min(self.W / w, self.H / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img_resized = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "        background = Image.new(\"RGB\", (self.W, self.H), self.fill)\n",
    "        paste_x = (self.W - new_w) // 2\n",
    "        paste_y = (self.H - new_h) // 2\n",
    "        background.paste(img_resized, (paste_x, paste_y))\n",
    "        return background\n",
    "\n",
    "# Photometric (non-geometric) augmentations\n",
    "photo_augs = [\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "]\n",
    "\n",
    "# 1) Super-charged train transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        ResizePad((224, 400)),\n",
    "        *photo_augs,\n",
    "        transforms.RandomAffine(\n",
    "            degrees=5,\n",
    "            translate=(0.1, 0.1),\n",
    "            scale=(0.8, 1.2),\n",
    "            fill=(int(0.485*255), int(0.456*255), int(0.406*255))\n",
    "        ),\n",
    "        transforms.RandomPerspective(\n",
    "            distortion_scale=0.2,\n",
    "            p=0.5,\n",
    "            fill=(int(0.485*255), int(0.456*255), int(0.406*255))\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        ResizePad((224, 400)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        ResizePad((224, 400)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '8apr_dataset_split'\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        image_datasets['train'], batch_size=16, shuffle=True, num_workers=6\n",
    "    ),\n",
    "    'val': torch.utils.data.DataLoader(\n",
    "        image_datasets['val'],   batch_size=16, shuffle=True, num_workers=6\n",
    "    ),\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        image_datasets['test'],  batch_size=16, shuffle=False, num_workers=6\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e3a0a-6bce-46e0-a807-c23a0b9e3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA if available; otherwise fallback to CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc67f15-d0d8-47bb-8036-7c3063b72914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class-to-Index Mapping:\")\n",
    "print(image_datasets['train'].class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049001d-8668-41ba-b40a-b5c35df52aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))  # CHW -> HWC\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean  # unnormalize\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=10)\n",
    "\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Display a batch of training data (optional)\n",
    "inputs, classes_batch = next(iter(dataloaders['train']))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27509694-703c-479d-b007-276c99c1e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=100, save_dir='saved_models'):\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    best_model_params_path = os.path.join(save_dir, 'best_model_params.pt')\n",
    "    best_acc = 0.0\n",
    "    since = time.time()\n",
    "    \n",
    "    # Save initial model (optional)\n",
    "    torch.save(model.state_dict(), best_model_params_path)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Adjust scheduler after validation phase\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            # Save the best model (based on validation accuracy)\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd04aa-fd86-46a3-8ce8-4c0ac42a89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(model, phase='val', num_images=6):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "\n",
    "    # Set rows and cols based on num_images\n",
    "    cols = 3\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols * 6, rows * 6))  # Bigger canvas (try 6 or even 7)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(rows, cols, images_so_far)\n",
    "                ax.axis('off')\n",
    "\n",
    "                pred_label = class_names[preds[j]]\n",
    "                true_label = class_names[labels[j]]\n",
    "\n",
    "                ax.set_title(f'Pred: {pred_label}\\nTrue: {true_label}',\n",
    "                             color='red' if pred_label != true_label else 'green')\n",
    "\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    return\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfd7cf-5c60-4787-bcd0-d0705f57daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 1. Load and modify EfficientNet-B0 ---\n",
    "model_ft = efficientnet_b0(weights='DEFAULT')\n",
    "\n",
    "# The original classifier is [Dropout(p=0.2), Linear(in_features, out_features)]\n",
    "# We’ll replace it with our own dropout + linear head:\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_ftrs, len(class_names))\n",
    ")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# --- 2. Loss function ---\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# === PHASE 1: Train head only (backbone frozen) ===\n",
    "print(\"=== PHASE 1: training head only ===\")\n",
    "# a) Freeze the backbone\n",
    "for param in model_ft.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# b) Optimizer for head only\n",
    "optimizer_head = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_ft.parameters()),\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# c) Scheduler for head (optional)\n",
    "scheduler_head = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_head, mode='min', patience=3, factor=0.5\n",
    ")\n",
    "\n",
    "# d) Train head for 23 epochs\n",
    "model_ft = train_model(\n",
    "    model=model_ft,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_head,\n",
    "    scheduler=scheduler_head,\n",
    "    num_epochs=44,\n",
    "    save_dir='saved_models/head_only'\n",
    ")\n",
    "\n",
    "\n",
    "# === PHASE 2: Fine-tuning full model ===\n",
    "print(\"\\n=== PHASE 2: fine-tuning full model ===\")\n",
    "# a) Unfreeze all layers\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# b) Optimizer over all params, with lower LR + weight decay\n",
    "optimizer_full = optim.AdamW(\n",
    "    model_ft.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# c) Scheduler for full model\n",
    "scheduler_full = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_full, mode='min', patience=5, factor=0.5\n",
    ")\n",
    "\n",
    "# d) Fine-tune for 44 epochs\n",
    "model_ft = train_model(\n",
    "    model=model_ft,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_full,\n",
    "    scheduler=scheduler_full,\n",
    "    num_epochs=90,\n",
    "    save_dir='saved_models/full_finetune'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Visualize results ---\n",
    "visualize_dataset(model_ft, phase='val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25215002-a7d4-4965-9e8e-537e9eb17876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_images_from_folders(model, class_names, max_images=8):\n",
    "    import os, glob, math\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # collect all test images\n",
    "    image_paths = glob.glob('8apr_dataset_split/test/*/*.*')\n",
    "    if not image_paths:\n",
    "        print(\"No test images found.\")\n",
    "        return\n",
    "\n",
    "    # only show up to max_images\n",
    "    image_paths = image_paths[:max_images]\n",
    "\n",
    "    images_per_row = 3\n",
    "    rows = math.ceil(len(image_paths) / images_per_row)\n",
    "    plt.figure(figsize=(images_per_row * 6, rows * 6))\n",
    "\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        true_label = os.path.basename(os.path.dirname(img_path))\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # preprocess & predict\n",
    "        input_tensor = data_transforms['val'](img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor).argmax(1).item()\n",
    "            predicted_label = class_names[pred]\n",
    "\n",
    "        # plot\n",
    "        ax = plt.subplot(rows, images_per_row, idx + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        color = 'green' if predicted_label == true_label else 'red'\n",
    "        ax.set_title(f'Pred: {predicted_label}\\nTrue: {true_label}', color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# call it:\n",
    "visualize_test_images_from_folders(model_ft, class_names, max_images=36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e102d-2cd3-489f-9bc6-d97d9fa456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# --------------------------------\n",
    "# 1. Define angle classes (must match your training folders and labels)\n",
    "class_to_angle = {\n",
    "    0: -2.5,\n",
    "    1: -5,\n",
    "    2: 0,\n",
    "    3: 2.5,\n",
    "    4: 5,\n",
    "}\n",
    "\n",
    "# --------------------------------\n",
    "# 2. Define the same transforms used during validation/testing\n",
    "data_transforms = {\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((400, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Pad((0, 88)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Geometric crop utilities from your augmentation script\n",
    "def largest_rotated_rect(w, h, angle_rad):\n",
    "    if w <= 0 or h <= 0:\n",
    "        return 0, 0\n",
    "    width_is_longer = w >= h\n",
    "    side_long, side_short = (w, h) if width_is_longer else (h, w)\n",
    "    sin_a = abs(math.sin(angle_rad))\n",
    "    cos_a = abs(math.cos(angle_rad))\n",
    "\n",
    "    if side_short <= 2.0 * sin_a * cos_a * side_long or abs(sin_a - cos_a) < 1e-10:\n",
    "        x = 0.5 * side_short\n",
    "        if width_is_longer:\n",
    "            wr = x / sin_a\n",
    "            hr = x / cos_a\n",
    "        else:\n",
    "            wr = x / cos_a\n",
    "            hr = x / sin_a\n",
    "    else:\n",
    "        cos_2a = cos_a ** 2 - sin_a ** 2\n",
    "        wr = (w * cos_a - h * sin_a) / cos_2a\n",
    "        hr = (h * cos_a - w * sin_a) / cos_2a\n",
    "\n",
    "    return int(wr), int(hr)\n",
    "\n",
    "def rotate_and_crop(img, angle):\n",
    "    orig_w, orig_h = img.size\n",
    "    rotated = img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "\n",
    "    angle_rad = math.radians(angle)\n",
    "    new_w, new_h = largest_rotated_rect(orig_w, orig_h, angle_rad)\n",
    "\n",
    "    cx, cy = rotated.size[0] // 2, rotated.size[1] // 2\n",
    "    left = cx - new_w // 2\n",
    "    top = cy - new_h // 2\n",
    "    right = cx + new_w // 2\n",
    "    bottom = cy + new_h // 2\n",
    "    cropped = rotated.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped.resize((orig_w, orig_h), Image.LANCZOS)\n",
    "\n",
    "# --------------------------------\n",
    "# 4. Load the saved trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_trained_model():\n",
    "    model_ft = models.mobilenet_v3_small(weights='DEFAULT')\n",
    "    num_ftrs = model_ft.classifier[3].in_features\n",
    "\n",
    "    # Re-create the exact head you trained: Dropout → Linear\n",
    "    model_ft.classifier[3] = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(num_ftrs, len(class_to_angle))\n",
    "    )\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "    model_weights_path = os.path.join('saved_models', 'best_model_params.pt')\n",
    "    model_ft.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "    model_ft.eval()\n",
    "    return model_ft\n",
    "\n",
    "# --------------------------------\n",
    "# 5. Rotate, correct, save and visualize ALL images\n",
    "def rotate_and_visualize(model, custom_data_dir, corrected_data_dir):\n",
    "    os.makedirs(corrected_data_dir, exist_ok=True)\n",
    "    image_paths = glob.glob(os.path.join(custom_data_dir, '*.*'))\n",
    "\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in '{custom_data_dir}'\")\n",
    "        return\n",
    "\n",
    "    total_images = len(image_paths)\n",
    "    plt.figure(figsize=(12, total_images * 3))\n",
    "\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = data_transforms['val'](img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            predicted_angle = class_to_angle[pred.item()]\n",
    "\n",
    "        corrected_img = rotate_and_crop(img, predicted_angle)\n",
    "\n",
    "        corrected_filename = f\"corrected_{os.path.basename(img_path)}\"\n",
    "        corrected_path = os.path.join(corrected_data_dir, corrected_filename)\n",
    "        corrected_img.save(corrected_path)\n",
    "\n",
    "        print(f\"[{idx+1}] {os.path.basename(img_path)} \"\n",
    "              f\"→ Predicted tilt: {predicted_angle}° \"\n",
    "              f\"→ Saved: {corrected_filename}\")\n",
    "\n",
    "        ax1 = plt.subplot(total_images, 2, idx * 2 + 1)\n",
    "        ax1.imshow(img)\n",
    "        ax1.set_title(f'Original\\nPred: {predicted_angle}°')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(total_images, 2, idx * 2 + 2)\n",
    "        ax2.imshow(corrected_img)\n",
    "        ax2.set_title('Corrected')\n",
    "        ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------\n",
    "# 6. Set device and run everything\n",
    "model_ft = load_trained_model()\n",
    "rotate_and_visualize(\n",
    "    model=model_ft,\n",
    "    custom_data_dir='custom_data',           # Folder with test/tilted images\n",
    "    corrected_data_dir='corrected_images'    # Output folder for corrected images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01529d-af6a-4ea5-aa86-2b39d8a121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = glob.glob('8apr_dataset_split/test/*/*.jpg')\n",
    "print(f\"Found {len(image_paths)} test images.\")\n",
    "print(\"\\nSample paths:\")\n",
    "print(\"\\n\".join(image_paths[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9eea9-c4d1-430f-9534-6c0d2b7e5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class-to-Index Mapping:\")\n",
    "print(image_datasets['train'].class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35d95b-0c88-4725-88b0-86815394f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for phase in ['train','val']:\n",
    "    labels = [label for (_,label) in image_datasets[phase].imgs]\n",
    "    print(phase, Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d248b-84ee-4810-9f34-cf00883cd11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
